{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybfev1A8wxS8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JahQmtn3wz2G"
   },
   "outputs": [],
   "source": [
    "def default_function(row):\n",
    "    ## returning 0 for firms with no defaults\n",
    "    if pd.isnull(row['def_date']):\n",
    "        return 0\n",
    "    \n",
    "    ## checking for a default within 12 months of April of next firm year;\n",
    "    ## as discussed in the 'When does a firm year end' sidebar,\n",
    "    ## financial statement data for a given year is not actually available until ~March/April of the next year\n",
    "    diff_default = row['def_date'] - row['stmt_date']\n",
    "    \n",
    "    if diff_default <= datetime.timedelta(days=486) and diff_default > datetime.timedelta(days=120):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def reduce_ratio_dimentionality(df, ratio_type, new, preproc_params):\n",
    "  if new:\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(df)\n",
    "    preproc_params[\"pca\"+ratio_type] = pca\n",
    "    result = pca.transform(df)\n",
    "  else:\n",
    "    pca = preproc_params[\"pca\"+ratio_type]\n",
    "    result = pca.transform(df)\n",
    "  return result, preproc_params\n",
    "\n",
    "\n",
    "def preprocessor(df, preproc_params = {}, data_path=\"/content/drive/MyDrive/AMLF Project/Data/\", new = True):\n",
    "    cleaned_data = df.copy()\n",
    "    \n",
    "    ## dropping column with no non-null values\n",
    "    cleaned_data = cleaned_data.drop(columns='eqty_corp_family_tot')\n",
    "\n",
    "    date_cols = ['stmt_date', 'def_date']\n",
    "\n",
    "    cleaned_data[date_cols[0]] = pd.to_datetime(cleaned_data[date_cols[0]], format='%Y-%m-%d')\n",
    "    cleaned_data[date_cols[1]] = pd.to_datetime(cleaned_data[date_cols[1]], format='%d/%m/%Y')\n",
    "\n",
    "    ## filling in NAs for margin_fin and roe by using definition\n",
    "\n",
    "    cleaned_data.loc[cleaned_data.margin_fin.isna(), 'margin_fin'] = cleaned_data['eqty_tot'] - cleaned_data['asst_current']\n",
    "    cleaned_data.loc[cleaned_data.roe.isna(), 'roe'] = cleaned_data['prof_operations'] / cleaned_data['eqty_tot']\n",
    "    \n",
    "    ## Filling missing data with 0s for HQ_City (categorical) and the fs categories that would be nan for being 0,\n",
    "    ## based on data definitions.\n",
    "    ## As well, Converting categorical columns to categorical type\n",
    "\n",
    "    cleaned_data['HQ_city'] = cleaned_data['HQ_city'].fillna(0).astype('category')\n",
    "    cleaned_data['legal_struct'] = cleaned_data['legal_struct'].astype('category')\n",
    "    cleaned_data['ateco_sector'] = cleaned_data['ateco_sector'].astype('category')\n",
    "\n",
    "    ## calculating inventory from definition\n",
    "    cleaned_data['inventory'] = cleaned_data['asst_current'] - cleaned_data['cash_and_equiv'] - cleaned_data['AR']\n",
    "    \n",
    "    ## building liquidity ratios\n",
    "    cleaned_data['current_ratio'] = cleaned_data['asst_current'] / cleaned_data['debt_st']\n",
    "    cleaned_data['quick_ratio'] = (cleaned_data['cash_and_equiv'] + cleaned_data['AR'] \n",
    "                                  -  cleaned_data['AP_st'])/ cleaned_data['debt_st']\n",
    "    cleaned_data['cash_ratio'] = cleaned_data['cash_and_equiv'] / cleaned_data['debt_st']\n",
    "    cleaned_data['cfo_ratio'] = cleaned_data['cf_operations'] / cleaned_data['debt_st']\n",
    "    cleaned_data['defensive_interval'] = 365 * (cleaned_data['cash_and_equiv'] + \n",
    "                                                cleaned_data['AR'] -  cleaned_data['AP_st']) / cleaned_data['COGS']\n",
    "    liquidity_ratio_cols = ['current_ratio', 'quick_ratio', 'cash_ratio', 'cfo_ratio', 'defensive_interval']                                            \n",
    "\n",
    "    ## building activity ratios\n",
    "    ## get rid of avg_days color \n",
    "    cleaned_data['receivables_turnover'] = cleaned_data['rev_operating'] / cleaned_data['AR']\n",
    "    cleaned_data['inventory_turnover'] = cleaned_data['COGS'] / cleaned_data['inventory']\n",
    "    cleaned_data['payables_turnover'] = (cleaned_data['COGS'] + cleaned_data['inventory']) / cleaned_data['AP_st']\n",
    "    cleaned_data['operating_cycle'] = ((365 / cleaned_data['receivables_turnover']) + \n",
    "                                       (365 / cleaned_data['inventory_turnover']))\n",
    "    cleaned_data['net_cash_cycle'] = cleaned_data['operating_cycle'] - (365 / cleaned_data['payables_turnover'])\n",
    "    cleaned_data['working_capital_turnover'] = cleaned_data['rev_operating'] / cleaned_data['wc_net']\n",
    "    activity_ratio_cols = ['receivables_turnover', 'inventory_turnover', 'payables_turnover', 'net_cash_cycle', \n",
    "                           'working_capital_turnover']\n",
    "\n",
    "    ## building solvency ratios\n",
    "    cleaned_data['debt_to_total_assets'] = (cleaned_data['debt_st'] + cleaned_data['debt_lt']) / cleaned_data['asst_tot']\n",
    "    cleaned_data['debt_to_equity'] = (cleaned_data['debt_st'] + cleaned_data['debt_lt']) / cleaned_data['eqty_tot']\n",
    "    cleaned_data['financial_leverage'] = cleaned_data['asst_tot'] / cleaned_data['eqty_tot']\n",
    "    cleaned_data['debt_service_coverage'] = cleaned_data['prof_operations'] / cleaned_data['debt_st']\n",
    "    cleaned_data['cfo_to_debt'] = cleaned_data['cf_operations'] / (cleaned_data['debt_st'] + cleaned_data['debt_lt'])\n",
    "    cleaned_data['cfo_to_operating_earnings'] = cleaned_data['cf_operations'] / cleaned_data['prof_operations']\n",
    "    solvency_ratio_cols =  ['debt_to_total_assets', 'debt_to_equity', 'financial_leverage', 'debt_service_coverage',\n",
    "                            'cfo_to_debt', 'cfo_to_operating_earnings']\n",
    "\n",
    "    ## building profitability ratios\n",
    "    cleaned_data['roic'] = (cleaned_data['prof_operations'] - \n",
    "                            cleaned_data['taxes']) / (cleaned_data['wc_net'] + cleaned_data['asst_intang_fixed'] +\n",
    "                                                      cleaned_data['asst_tang_fixed'] + cleaned_data['asst_fixed_fin'])\n",
    "    cleaned_data['operating_margin'] = cleaned_data['ebitda'] / cleaned_data['rev_operating']\n",
    "    cleaned_data['gross_profit_margin'] = (cleaned_data['rev_operating'] - \n",
    "                                           cleaned_data['COGS']) / cleaned_data['rev_operating']\n",
    "    cleaned_data['net_profit_margin_on_sales'] = cleaned_data['profit'] / cleaned_data['rev_operating']\n",
    "    cleaned_data['cash_roa'] = cleaned_data['cf_operations'] / cleaned_data['asst_tot']\n",
    "    profitability_ratio_cols = ['roe', 'roic', 'operating_margin', 'gross_profit_margin', \n",
    "                                'net_profit_margin_on_sales', 'cash_roa']\n",
    "\n",
    "    ## Assembling list of ratios\n",
    "    fs_ratio_cols = liquidity_ratio_cols + activity_ratio_cols + solvency_ratio_cols + profitability_ratio_cols\n",
    "\n",
    "    ## Replacing infinite values with NaNs\n",
    "    cleaned_data = cleaned_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    ## imputing missing values for fs_ratio_cols using (train data) industry means\n",
    "    if new:\n",
    "      ## creating dict of grouped means\n",
    "      preproc_params['fs_ratio_group_means'] = {}\n",
    "      ## iterating through ratio cols\n",
    "      for ratio_col in fs_ratio_cols:\n",
    "        ## creating series of grouped means for ratio cols\n",
    "        preproc_params['fs_ratio_group_means'][ratio_col] = cleaned_data.groupby('ateco_sector')[ratio_col].mean()\n",
    "        ## filling NAs in each group with the mean\n",
    "        for group in preproc_params['fs_ratio_group_means'][ratio_col].index:\n",
    "          cleaned_data.loc[(cleaned_data[ratio_col].isnull()) & (cleaned_data['ateco_sector'] == group), \n",
    "                           ratio_col] = preproc_params['fs_ratio_group_means'][ratio_col][group]\n",
    "    else:\n",
    "      ## iterating through ratio cols\n",
    "      for ratio_col in preproc_params['fs_ratio_group_means'].keys():\n",
    "        ## filling NAs in each group with the mean\n",
    "        for group in preproc_params['fs_ratio_group_means'][ratio_col].index:\n",
    "          cleaned_data.loc[(cleaned_data[ratio_col].isnull()) & (cleaned_data['ateco_sector'] == group), \n",
    "                           ratio_col] = preproc_params['fs_ratio_group_means'][ratio_col][group]\n",
    "\n",
    "    ## Doing Principal Component Analysis on each of the ratios to reduce dimentionality\n",
    "    liquidity_ratio_df = cleaned_data[liquidity_ratio_cols].copy()\n",
    "    activity_ratio_cols_df = cleaned_data[activity_ratio_cols].copy()\n",
    "    solvency_ratio_cols_df = cleaned_data[solvency_ratio_cols].copy()\n",
    "    profitability_ratio_cols_df = cleaned_data[profitability_ratio_cols].copy()\n",
    "\n",
    "    liquidity_ratio_pca, preproc_params = reduce_ratio_dimentionality(liquidity_ratio_df, \"liquidity\", new, preproc_params)\n",
    "    activity_ratio_pca, preproc_params = reduce_ratio_dimentionality(activity_ratio_cols_df, \"activity\", new, preproc_params)\n",
    "    solvency_ratio_pca, preproc_params = reduce_ratio_dimentionality(solvency_ratio_cols_df, \"solvency\", new, preproc_params)\n",
    "    profitability_ratio_pca, preproc_params = reduce_ratio_dimentionality(profitability_ratio_cols_df, \"profitability\", new, preproc_params)\n",
    "    cleaned_data['liquidity_ratio'] = liquidity_ratio_pca\n",
    "    cleaned_data['activity_ratio'] = activity_ratio_pca\n",
    "    cleaned_data['solvency_ratio'] = solvency_ratio_pca\n",
    "    cleaned_data['profitability_ratio'] = profitability_ratio_pca\n",
    "\n",
    "    ## Assembling list of reduced ratios\n",
    "    reduced_ratio_cols = ['liquidity_ratio', 'activity_ratio', 'solvency_ratio', 'profitability_ratio']\n",
    "    \n",
    "    ## integrating economic data\n",
    "    italy_econ_data = pd.read_csv(data_path + \"italy_economic_data.csv\")\n",
    "    italy_econ_data['Date'] = pd.to_datetime(italy_econ_data['Date'])\n",
    "    italy_econ_data = italy_econ_data.set_index('Date')\n",
    "    cleaned_data = pd.merge(cleaned_data, italy_econ_data,\n",
    "                            left_on='stmt_date', right_index=True)\n",
    "    italy_econ_cols = list(italy_econ_data.columns)\n",
    "    \n",
    "    #categorical variables\n",
    "    cat_cols = ['legal_struct', 'ateco_sector']\n",
    "    categorical_columns = []\n",
    "    \n",
    "    \n",
    "    if new:\n",
    "        one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        one_hot_encoder.fit(cleaned_data[cat_cols])\n",
    "        categories = one_hot_encoder.categories_\n",
    "        dummies = pd.DataFrame(one_hot_encoder.transform(cleaned_data[cat_cols]).toarray())\n",
    "        categorical_columns=[]\n",
    "        for j,col in enumerate(cat_cols):\n",
    "          \n",
    "          curr_col_categories = categories[j]\n",
    "          new_columns_names = [col+'_'+str(i) for i in curr_col_categories]\n",
    "          categorical_columns += new_columns_names\n",
    "          cleaned_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "        dummies.columns = categorical_columns\n",
    "        cleaned_data = pd.concat([cleaned_data, dummies], axis=1)\n",
    "        preproc_params['one_hot_encoder'] = one_hot_encoder\n",
    "    else:\n",
    "        one_hot_encoder = preproc_params['one_hot_encoder']\n",
    "        categories = one_hot_encoder.categories_\n",
    "        dummies = pd.DataFrame(one_hot_encoder.transform(cleaned_data[cat_cols]).toarray())\n",
    "        categorical_columns=[]\n",
    "        for j,col in enumerate(cat_cols):\n",
    "          curr_col_categories = categories[j]\n",
    "          new_columns_names = [col+'_'+str(i) for i in curr_col_categories]\n",
    "          categorical_columns += new_columns_names\n",
    "          cleaned_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "        dummies.columns = categorical_columns\n",
    "        cleaned_data = pd.concat([cleaned_data, dummies], axis=1)\n",
    "\n",
    "    if new:\n",
    "      ## building outcome variable using default_function\n",
    "      cleaned_data['defaulted_within_12_months'] = cleaned_data.apply(lambda x: default_function(x), axis=1)\n",
    "      del cleaned_data['def_date']\n",
    "\n",
    "      true_pd = cleaned_data['defaulted_within_12_months'].mean()\n",
    "      preproc_params['true_pd'] = true_pd\n",
    "\n",
    "    data_cols = [col for col in cleaned_data.columns if\n",
    "                 col in reduced_ratio_cols + italy_econ_cols + categorical_columns + ['defaulted_within_12_months']]\n",
    "    cleaned_data = cleaned_data[data_cols]\n",
    "    \n",
    "    num_features = reduced_ratio_cols + italy_econ_cols\n",
    "\n",
    "    ## Standardizing data\n",
    "    if new:\n",
    "      scaler = RobustScaler()\n",
    "      cleaned_data[num_features] = scaler.fit_transform(cleaned_data[num_features])\n",
    "      preproc_params['scaler'] = scaler\n",
    "    else:\n",
    "      scaler = preproc_params['scaler']\n",
    "      cleaned_data[num_features] = scaler.transform(cleaned_data[num_features])\n",
    "      \n",
    "    return cleaned_data, preproc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSaP8U_AxGpO"
   },
   "outputs": [],
   "source": [
    "def create_calibrator(true_pd, sample_set_pd):\n",
    "  \n",
    "\n",
    "  '''\n",
    "  Function to create a calibrator:\n",
    "  Arguments:\n",
    "    true_pd = True probability of default\n",
    "    sample_set = sample set probability of default\n",
    "  '''\n",
    "  def calibrator(model_pd):\n",
    "\n",
    "    corrected_pd = ((true_pd)*(model_pd - model_pd*sample_set_pd))/(sample_set_pd - model_pd*sample_set_pd + model_pd*true_pd - sample_set_pd*true_pd)\n",
    "\n",
    "    return corrected_pd\n",
    "\n",
    "  return calibrator\n",
    "\n",
    "def estimator(cleaned_data, fitting_algo, calibrator, est_params = {'max_depth':60, 'max_leaf_nodes':1000, 'target_name':'defaulted_within_12_months'}):\n",
    "\n",
    "    model = fitting_algo(max_depth = est_params['max_depth'])\n",
    "\n",
    "    X = cleaned_data.loc[:, cleaned_data.columns!=est_params['target_name']]\n",
    "    y = cleaned_data[est_params['target_name']]\n",
    "    model.fit(X,y)\n",
    "\n",
    "    y_pred_proba = model.predict_proba(X)[:,1]\n",
    "    y_pred = model.predict(X)\n",
    "    y_pred_proba_calibrated = calibrator(y_pred_proba)\n",
    "\n",
    "\n",
    "    # print('Before Calibration:')\n",
    "    # draw_calibration_curve(y, y_pred_proba)\n",
    "    # print('After Calibration:')\n",
    "    # draw_calibration_curve(y, y_pred_proba_calibrated)\n",
    "\n",
    "    print('Area under ROC curve:', roc_auc_score(y, y_pred_proba))\n",
    "    fpr,tpr,threshs = roc_curve(y, y_pred_proba)\n",
    "    ConfusionMatrixDisplay.from_predictions(y, y_pred)\n",
    "    plt.show()   \n",
    "\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "\n",
    "    return(model)\n",
    "\n",
    "def predictor(new_df, model, calibrator):\n",
    "    # your code here\n",
    "    predictions = model.predict(new_df)\n",
    "    predictions_proba = model.predict_proba(new_df)[:,1]\n",
    "    calibrated_predictions_proba = np.array([calibrator(a) for a in predictions_proba])\n",
    "\n",
    "    return(predictions,calibrated_predictions_proba)\n",
    "\n",
    "def test_harness(df, preprocessor=preprocessor, estimator=estimator, predictor=predictor, data_path=None):     \n",
    "    new_df = df.copy()\n",
    "\n",
    "    if data_path is None:\n",
    "      print(\"Please give Data Path\")\n",
    "      return None\n",
    "\n",
    "    ## load saved model\n",
    "    with open(data_path + 'model.pkl' , 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    ## load save preprocessing parameters\n",
    "    with open(data_path + 'preproc_params.pkl', 'rb') as fid:\n",
    "        preproc_params = pickle.load(fid)\n",
    "    \n",
    "    ## making sure data is in chronological order\n",
    "    new_df = new_df.sort_values(preproc_params['year_col'])\n",
    "\n",
    "    sample_set_pd = preproc_params['sample_set_pd']\n",
    "    true_pd = preproc_params['true_pd']\n",
    "    \n",
    "    calibrator = create_calibrator(true_pd, sample_set_pd)\n",
    "    ## Preprocessing the test data\n",
    "    preproc_test_set, test_preproc_params = preprocessor(df, preproc_params, new=False, data_path=data_path)\n",
    "    \n",
    "    ## Predicting the default and getting the probabilities\n",
    "    predictions, pred_proba = predictor(preproc_test_set, model, calibrator)\n",
    "    \n",
    "    return pred_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMyt2vZY0eAr",
    "outputId": "f5f50cdd-9d5f-40eb-825b-11e3192e2afa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Kindly install \"imbalanced-learn\" package\n",
    "## Eg. conda install -c conda-forge imbalanced-learn\n",
    "## We would also require python3.7 to run\n",
    "\n",
    "## Path where all the data is stored\n",
    "data_path = \"./\"\n",
    "## load the test data\n",
    "test_data = pd.read_csv(data_path+'train.csv')\n",
    "#del test_data['def_date']\n",
    "\n",
    "predicted_probabilities = test_harness(test_data, data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 | packaged by conda-forge | (default, Sep 29 2021, 19:24:02) \n[Clang 11.1.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aca298eec0c38957b0c2896ae72a2fdf8bb1f6280a2ff0b2db24b04965136b61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
